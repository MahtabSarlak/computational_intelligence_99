{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Hosh_final_proj.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FtotXSbTVBq",
        "outputId": "7b268d4b-94b7-4b70-b663-55cda0caa18b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCZt-7BkUmuo",
        "outputId": "39bd375c-7c6a-4ef6-9af8-e240109223bb"
      },
      "source": [
        "\r\n",
        "!pip install hazm\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import cv2 as cv\r\n",
        "import pandas as pd\r\n",
        "from hazm import *\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "import math\r\n",
        "import re\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\r\n",
        "from sklearn import linear_model\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.linear_model import SGDClassifier\r\n",
        "from sklearn.metrics import precision_score, accuracy_score, recall_score\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from string import punctuation\r\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\r\u001b[K     |█                               | 10kB 18.6MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 20.1MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 10.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 51kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 71kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 81kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 102kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 112kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 122kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 133kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 143kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 153kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 163kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 174kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 184kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 194kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 204kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 215kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 225kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 235kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 245kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 256kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 266kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 276kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 286kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 296kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 307kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 7.6MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 12.0MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 15.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394469 sha256=f7eeafbf4e19d927ec34c51e32fb377b8d0c486ffc80acf48aaa1df61a320b96\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp36-cp36m-linux_x86_64.whl size=154538 sha256=c4c233f78889380e6fc7a2da9eacfbf08e0f6ca2cc5e26827a991999751df62e\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKL_6QXITYRY"
      },
      "source": [
        "url = 'drive/My Drive/train.csv'\r\n",
        "df = pd.read_csv(url, engine='python',encoding='utf-8', error_bad_lines=False)\r\n",
        "test_url = 'drive/My Drive/test.csv'\r\n",
        "df_test = pd.read_csv(test_url, engine='python',encoding='utf-8', error_bad_lines=False)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "jcnZnYVCUYt8",
        "outputId": "481d1f3b-d9c9-46c3-f58f-c9fa3bf98f37"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Text</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>\\nخبرنامه دانشگاه علم و صنعت ايران \\nشماره ياز...</td>\n",
              "      <td>Science and Culture</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\\nتا پايان سال 1378 دهها زمين فوتبال و \\nسالن ...</td>\n",
              "      <td>Sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>\\nانجمن توليدكنندگان تجهيزات صنعت نفت تشكيل شد...</td>\n",
              "      <td>Economy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\\nكرتين براي سومين بار نخست وزير كانادا \\nشد \\...</td>\n",
              "      <td>Miscellaneous.World News</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\\nخداحافظ رفقا \\nنمايندگان اروپاي شرقي در جام ...</td>\n",
              "      <td>Sport</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                  Category\n",
              "0           0  ...       Science and Culture\n",
              "1           1  ...                     Sport\n",
              "2           2  ...                   Economy\n",
              "3           3  ...  Miscellaneous.World News\n",
              "4           4  ...                     Sport\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "ykYjadiUs8Eb",
        "outputId": "6d0d338a-4610-40b1-ff1c-1f802e0b844d"
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>\\nهفت اقليم \\nآلودگي هوا پكن را تهديد مي كند \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\\nگل و گياه زعفران زينتي \\nنام علمي: Crocus ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>\\nيادداشت \\nقانون بودجه و صنايع كوچك \\nدر شمار...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\\nدر سالروز ميلاد حضرت مهدي \\nهمايش ادبي دانش ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\\nاز IRA تا فارك \\n بوگوتا، پايتخت پرهرج ومرج ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id                                               Text\n",
              "0   0  \\nهفت اقليم \\nآلودگي هوا پكن را تهديد مي كند \\...\n",
              "1   1  \\nگل و گياه زعفران زينتي \\nنام علمي: Crocus ba...\n",
              "2   2  \\nيادداشت \\nقانون بودجه و صنايع كوچك \\nدر شمار...\n",
              "3   3  \\nدر سالروز ميلاد حضرت مهدي \\nهمايش ادبي دانش ...\n",
              "4   4  \\nاز IRA تا فارك \\n بوگوتا، پايتخت پرهرج ومرج ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-oKS8YJXbo4",
        "outputId": "4e75b5be-f137-4e1c-e240-25bbff4dbc07"
      },
      "source": [
        "train_len = df.shape[0]\r\n",
        "print(train_len)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "150096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S46_F9F4KHe",
        "outputId": "1c854fbe-d272-4ba3-9a2f-401b1a5f9b47"
      },
      "source": [
        "test_len = df_test.shape[0]\r\n",
        "print(test_len)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCPxhyoUxZu4"
      },
      "source": [
        "# *Preprocessing*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV1VfzubfLTm",
        "outputId": "87799cc9-f615-4c86-9f35-59dbbc2851e3"
      },
      "source": [
        "normalizer = Normalizer()\r\n",
        "lemmatizer = Lemmatizer()\r\n",
        "stemmer = Stemmer()\r\n",
        "\r\n",
        "stop_words= []\r\n",
        "with open(\"drive/My Drive/stopwords.txt\" ,encoding = 'utf-8' ) as f:\r\n",
        "    stop_words = f.read().splitlines()\r\n",
        "stopwords = stop_words [1:]\r\n",
        "print(stopwords)\r\n",
        "\r\n",
        "\r\n",
        "def preprocessing(text):\r\n",
        "    # text = normalizer.normalize(text)\r\n",
        "    tokens = word_tokenize(text)\r\n",
        "    tokens = [word for word in tokens if not word in stopwords]\r\n",
        "    cleared_text = []\r\n",
        "    for word in tokens:\r\n",
        "          # word = normalizer.normalize(word)\r\n",
        "          word = re.sub(r'\\d+', '', word)\r\n",
        "          word = re.sub(r'\\s*[A-Za-z]+\\b', '', word)\r\n",
        "          cleared_text.append(word)\r\n",
        "    return cleared_text\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "  output = []\r\n",
        "  for i in range(len(text)):\r\n",
        "    if(i % 10000 == 0):\r\n",
        "      print(\"iteration # {}\".format(i))\r\n",
        "    clean_txt = preprocessing(text[i])\r\n",
        "    output.append(clean_txt)\r\n",
        "  return output"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\"', '#', '%', '\\\\', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '\\\\\\\\', '//', '?', '@', '\\\\', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '’', '«', '»', '”', '“', '′', '‘', '،', '\\\\\\\\', 'و', 'در', 'به', 'از', 'که', 'این', 'را', 'با', 'است', 'برای', 'آن', 'یک', 'خود', 'تا', 'کرد', 'بر', 'هم', 'نیز', 'گفت', 'می\\u200cشود', 'وی', 'شد', 'دارد', 'ما', 'اما', 'یا', 'شده', 'باید', 'هر', 'آنها', 'بود', 'او', 'دیگر', 'دو', 'مورد', 'می\\u200cکند', 'شود', 'کند', 'وجود', 'بین', 'پیش', 'شده_است', 'پس', 'نظر', 'اگر', 'همه', 'یکی', 'حال', 'هستند', 'من', 'کنند', 'گردد', 'حتما', 'سلام', 'دلم', 'دیدن', 'بزنید', 'صحبتهای', 'عنوان', 'معناست', 'خبرهای', 'شب', 'نیست', 'باشد', 'چه', 'عده\\u200cای', 'علیه\\u200cالسلام', 'باشیم', 'فعلی', 'صد', 'ان', 'انها', 'جناب', 'جناب\\u200cاقای', 'میشود', 'عده', 'بنا', 'دکتر', 'عرض', 'بایستی', 'نمایند', 'دست', 'بیش', 'روز\\u200cگذشته', 'اشاره', 'میگن', 'کنه', 'مارو', 'اونم', 'اینم', 'فک', 'کردین', 'بابا', 'بدید', 'اخه', 'باشن', 'هستن', 'اومده', 'نباشه', 'خدایی', 'دارین', 'میشن', 'یکم', 'هستین', 'هستن', 'مارو', 'کنن', 'ادم', 'اینه', 'اقای', 'بی', 'می', 'بخش', 'می\\u200cکنند', 'همین', 'افزود', 'هایی', 'های', 'ای', 'نمی', 'ام', 'دارند', 'راه', 'همچنین', 'روی', 'داد', 'بیشتر', 'بسیار', 'سه', 'داشت', 'چند', 'سوی', 'تنها', 'هیچ', 'میان', 'اینکه', 'شدن', 'بعد', 'جدید', 'ولی', 'حتی', 'کردن', 'برخی', 'کردند', 'می\\u200cدهد', 'اول', 'نه', 'کرده_است', 'نسبت', 'شما', 'چنین', 'طور', 'افراد', 'تمام', 'درباره', 'بار', 'بسیاری', 'می\\u200cتواند', 'کرده', 'چون', 'ندارد', 'دوم', 'طی', 'حدود', 'همان', 'بدون', 'البته', 'آنان', 'می\\u200cگوید', 'دیگری', 'خواهد_شد', 'کنیم', 'قابل', 'یعنی', 'رشد', 'می\\u200cتوان', 'وارد', 'کل', 'ویژه', 'قبل', 'براساس', 'نیاز', 'گذاری', 'هنوز', 'لازم', 'سازی', 'بوده_است', 'چرا', 'می\\u200cشوند', 'وقتی', 'گرفت', 'کم', 'جای', 'حالی', 'تغییر', 'پیدا', 'اکنون', 'تحت', 'باعث', 'مدت', 'فقط', 'زیادی', 'تعداد', 'آیا', 'بیان', 'رو', 'شدند', 'عدم', 'کرده_اند', 'بودن', 'نوع', 'بلکه', 'جاری', 'دهد', 'برابر', 'مهم', 'بوده', 'اخیر', 'مربوط', 'امر', 'زیر', 'گیری', 'شاید', 'خصوص', 'آقای', 'اثر', 'کننده', 'بودند', 'فکر', 'کنار', 'اولین', 'سوم', 'سایر', 'کنید', 'ضمن', 'مانند', 'باز', 'می\\u200cگیرد', 'ممکن', 'حل', 'دارای', 'پی', 'مثل', 'می\\u200cرسد', 'اجرا', 'دور', 'منظور', 'کسی', 'موجب', 'طول', 'امکان', 'آنچه', 'تعیین', 'گفته', 'شوند', 'جمع', 'خیلی', 'علاوه', 'گونه', 'تاکنون', 'رسید', 'ساله', 'گرفته', 'شده_اند', 'علت', 'چهار', 'داشته_باشد', 'خواهد_بود', 'طرف', 'تهیه', 'تبدیل', 'مناسب', 'زیرا', 'مشخص', 'می\\u200cتوانند', 'نزدیک', 'جریان', 'روند', 'بنابراین', 'می\\u200cدهند', 'یافت', 'نخستین', 'بالا', 'پنج', 'ریزی', 'عالی', 'چیزی', 'نخست', 'بیشتری', 'ترتیب', 'شده_بود', 'شروع', 'فرد', 'می\\u200cرود', 'دهند', 'آخرین', 'دادن', 'بهترین', 'شامل', 'گیرد', 'بخشی', 'باشند', 'تمامی', 'بهتر', 'داده_است', 'حد', 'نبود', 'کسانی', 'می\\u200cکرد', 'داریم', 'می\\u200cباشد', 'دانست', 'ناشی', 'داشتند', 'دهه', 'می\\u200cشد', 'ایشان', 'آنجا', 'گرفته_است', 'دچار', 'می\\u200cآید', 'لحاظ', 'آنکه', 'داده', 'بعضی', 'هستیم', 'اند', 'برداری', 'نباید', 'می\\u200cکنیم', 'نشست', 'سهم', 'همیشه', 'آمد', 'اش', 'وگو', 'می\\u200cکنم', 'حداقل', 'طبق', 'جا', 'خواهد_کرد', 'نوعی', 'چگونه', 'رفت', 'هنگام', 'فوق', 'روش', 'ندارند', 'سعی', 'بندی', 'شمار', 'کلی', 'کافی', 'مواجه', 'همچنان', 'زیاد', 'سمت', 'کوچک', 'داشته_است', 'چیز', 'پشت', 'آورد', 'حالا', 'روبه', 'سال\\u200cهای', 'دادند', 'می\\u200cکردند', 'عهده', 'نیمه', 'جایی', 'دیگران', 'سی', 'بروز', 'یکدیگر', 'آمده_است', 'جز', 'کنم', 'سپس', 'کنندگان', 'خودش', 'همواره', 'یافته', 'شان', 'صرف', 'نمی\\u200cشود', 'رسیدن', 'چهارم', 'یابد', 'متر', 'ساز', 'داشته', 'کرده_بود', 'باره', 'نحوه', 'کردم', 'تو', 'شخصی', 'داشته_باشند', 'محسوب', 'پخش', 'کمی', 'سراسر', 'کاملا', 'داشتن', 'نظیر', 'آمده', 'گروهی', 'فردی', 'ع', 'ص', 'همچون', 'خویش', 'کدام', 'دسته', 'سبب', 'عین', 'آوری', 'متاسفانه', 'بیرون', 'دار', 'ابتدا', 'شش', 'افرادی', 'می\\u200cگویند', 'سالهای', 'درون', 'نیستند', 'یافته_است', 'پر', 'خاطرنشان', 'گاه', 'جمعی', 'اغلب', 'دوباره', 'می\\u200cیابد', 'لذا', 'گردد', 'اینجا', 'ها', '-', '_', 'خواهد', 'قرار', 'امروز', 'سال', 'انجام', 'تیم', 'شرکت', 'نفر', 'صورت', 'ماه', 'توسط', 'کار', 'فارس', 'پایان', 'ادامه', 'بزرگ', 'دلیل', 'آغاز', 'فوری', 'زمان', 'منتشر', 'بررسی', 'امور', 'کانال', 'نشان', 'دنبال', 'استفاده', 'فردا', 'علی', 'نام', 'افزایش', 'ارسال', 'صبح', 'ببینید', 'جلسه', 'شنبه', 'پیام', 'پروکسی', 'تر', 'هزار', '___', '__', 'دماوند']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j4lN_okk_ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2bef04-83f8-40a5-9624-ef001249163d"
      },
      "source": [
        "preprocessed_text_train = clean_text(df['Text'])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration # 0\n",
            "iteration # 10000\n",
            "iteration # 20000\n",
            "iteration # 30000\n",
            "iteration # 40000\n",
            "iteration # 50000\n",
            "iteration # 60000\n",
            "iteration # 70000\n",
            "iteration # 80000\n",
            "iteration # 90000\n",
            "iteration # 100000\n",
            "iteration # 110000\n",
            "iteration # 120000\n",
            "iteration # 130000\n",
            "iteration # 140000\n",
            "iteration # 150000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD76mme-cCVY",
        "outputId": "47ec9a9d-b09e-41cc-b734-38a239195285"
      },
      "source": [
        "preprocessed_text_test = clean_text(df_test['Text'])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration # 0\n",
            "iteration # 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvmyMMt9xV9A"
      },
      "source": [
        "# Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDeCzWkXjQLH"
      },
      "source": [
        "def dummy_fun(doc):\r\n",
        "    return doc\r\n",
        "def tfidf(data, ma = 0.6, mi = 0.0001):\r\n",
        "    tfidf_vectorize = TfidfVectorizer(\r\n",
        "    analyzer='word',\r\n",
        "    tokenizer=dummy_fun,\r\n",
        "    preprocessor=dummy_fun,\r\n",
        "    token_pattern=None,\r\n",
        "    max_df = ma,\r\n",
        "    min_df = mi) \r\n",
        "    tfidf_data = tfidf_vectorize.fit_transform(data)\r\n",
        "    return tfidf_data"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFqNAPUUcMt0",
        "outputId": "76ff98cc-f7c8-4f12-b35d-fcda12de4de3"
      },
      "source": [
        "word_embedded_text_train = tfidf(preprocessed_text_train)\r\n",
        "print(word_embedded_text_train.shape)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150096, 40382)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiKT6rKkcOfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84510d16-3017-431f-a7a0-44e9124a8e86"
      },
      "source": [
        "word_embedded_text_test = tfidf(preprocessed_text_test)\r\n",
        "print(word_embedded_text_test.shape)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16678, 47958)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en--QUqicSHV"
      },
      "source": [
        "merge_train_test = preprocessed_text_train\r\n",
        "for i in range(0 , len(preprocessed_text_test)):\r\n",
        "    merge_train_test.append(preprocessed_text_test[i])"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UutTshHcT5w"
      },
      "source": [
        "merge_word_embedded_text = tfidf(merge_train_test)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGy2RXFlFHEN"
      },
      "source": [
        "# Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzYzJwahgQO3"
      },
      "source": [
        "import sklearn\r\n",
        "def train_test_spiliting(X,Y):\r\n",
        "  x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size = 0.05, random_state = 42)\r\n",
        "  print(\"X_train lenght: {}\".format(x_train.shape[0]))\r\n",
        "  print(\"X_test lenght: {}\".format(x_test.shape[0]))\r\n",
        "  return x_train, x_test, y_train, y_test "
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxqzacXOdsAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ed3539-39b8-4912-de0d-7fe00bb2c50d"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_spiliting(merge_word_embedded_text[0:150096] , df['Category'])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train lenght: 142591\n",
            "X_test lenght: 7505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hoXU3j2xTWw"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDeXptQ7KwwZ"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "def test_logestic(x_train, x_test, y_train, y_test):\n",
        "    model = Pipeline([ ('clf', LogisticRegression(n_jobs = 1 , C = 1e5))])\n",
        "    classifier = model.fit(x_train,y_train)\n",
        "    predictions = classifier.predict(x_test)\n",
        "    print(classification_report(y_test, predictions))\n",
        "    a = accuracy_score(y_test, predictions)\n",
        "    p = precision_score(y_test, predictions, average = 'weighted')\n",
        "    r = recall_score(y_test, predictions, average = 'weighted')\n",
        "    return a , p, r ,classifier"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6TsM6fIxBns"
      },
      "source": [
        "def train(x_train, x_test, y_train, y_test):\r\n",
        "  accuracy, precision, recall , classifier = test_logestic(x_train, x_test, y_train, y_test)\r\n",
        "  print(\"model accuracy {}\".format(accuracy))\r\n",
        "  print(\"model precision {}\".format(precision))\r\n",
        "  print(\"model recall {}\".format(recall))\r\n",
        "  return classifier"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRKnP10OxFlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162206d7-65cc-4e4a-f496-84c1f90fae1b"
      },
      "source": [
        "classifier = train(x_train, x_test, y_train, y_test)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                                      precision    recall  f1-score   support\n",
            "\n",
            "                                                             Economy       0.81      0.87      0.84       874\n",
            "                                                 Economy.Agriculture       0.91      0.43      0.59        23\n",
            "                                             Economy.Bank and Bourse       0.60      0.69      0.64        51\n",
            "                                                    Economy.Commerce       0.67      0.14      0.24        14\n",
            "                                   Economy.Dwelling and Construction       0.00      0.00      0.00         3\n",
            "                                                    Economy.Industry       0.36      0.20      0.26        40\n",
            "                                                         Economy.Oil       0.75      0.75      0.75        48\n",
            "                                                  Literature and Art       0.62      0.60      0.61       238\n",
            "                                              Literature and Art.Art       0.52      0.51      0.51        51\n",
            "                                       Literature and Art.Art.Cinema       0.00      0.00      0.00        11\n",
            "                                        Literature and Art.Art.Music       0.00      0.00      0.00         4\n",
            "                                      Literature and Art.Art.Theater       0.00      0.00      0.00         1\n",
            "                                       Literature and Art.Literature       0.00      0.00      0.00         1\n",
            "                                                       Miscellaneous       0.76      0.78      0.77      1656\n",
            "                                            Miscellaneous.Happenings       0.77      0.72      0.75       158\n",
            "                                      Miscellaneous.Islamic Councils       0.70      0.47      0.56        34\n",
            "                                               Miscellaneous.Picture       0.93      0.96      0.94        26\n",
            "                                    Miscellaneous.Picture.Caricature       0.86      0.86      0.86         7\n",
            "                                                 Miscellaneous.Urban       0.74      0.74      0.74       838\n",
            "                                            Miscellaneous.World News       0.90      0.94      0.92       666\n",
            "                                                 Natural Environment       0.52      0.42      0.46        38\n",
            "                                                            Politics       0.82      0.82      0.82       892\n",
            "                                              Politics.Iran Politics       0.85      0.36      0.51        47\n",
            "                                                 Science and Culture       0.75      0.75      0.75       553\n",
            "                                         Science and Culture.Science       0.76      0.62      0.68        42\n",
            "                                    Science and Culture.Science.Book       0.75      0.18      0.29        17\n",
            "Science and Culture.Science.Information and Communication Technology       1.00      0.27      0.43        11\n",
            "                     Science and Culture.Science.Medicine and Remedy       0.00      0.00      0.00         2\n",
            "                                                              Social       0.65      0.68      0.66       357\n",
            "                                                     Social.Religion       0.55      0.50      0.52        22\n",
            "                                                        Social.Women       0.53      0.40      0.46        20\n",
            "                                                               Sport       0.96      0.97      0.97       729\n",
            "                                                     Sport.World Cup       0.40      0.18      0.25        11\n",
            "                                                             Tourism       0.50      0.50      0.50        20\n",
            "\n",
            "                                                            accuracy                           0.79      7505\n",
            "                                                           macro avg       0.59      0.48      0.51      7505\n",
            "                                                        weighted avg       0.78      0.79      0.78      7505\n",
            "\n",
            "model accuracy 0.7884077281812125\n",
            "model precision 0.784780410402406\n",
            "model recall 0.7884077281812125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX4Mpbc2xJzM"
      },
      "source": [
        "# Test Prediction\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHgibeEtGG80"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwJcxC5ar6J1"
      },
      "source": [
        "def prediction(word_embedded_text_test):\r\n",
        "  predictions = classifier.predict(word_embedded_text_test)\r\n",
        "  return predictions"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jMo4rsdsa22"
      },
      "source": [
        "predictions = prediction(merge_word_embedded_text[150096:])"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhWfac6zsOIn"
      },
      "source": [
        "import csv\r\n",
        "def save_prediction(predict,df_test):\r\n",
        "  myfile = open(\"result.csv\", \"w\")\r\n",
        "  writer = csv.writer(myfile)\r\n",
        "  writer.writerow([\"Id\", \"Category\"])\r\n",
        "\r\n",
        "  for i in range(len(df_test)):\r\n",
        "    writer.writerow([df_test['Id'][i], predict[i]])\r\n",
        "\r\n",
        "  myfile.close()"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yONJDNqusZTp"
      },
      "source": [
        "save_prediction(predictions,df_test)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kAXNsARn825"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}